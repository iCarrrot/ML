{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3gmko6zus4J"
   },
   "source": [
    "# Homework 3\n",
    "\n",
    "**For exercises in the week 11-16.12.19**\n",
    "\n",
    "**Points: 7 + 1b**\n",
    "\n",
    "Please solve the problems at home and bring to class a [declaration form](http://ii.uni.wroc.pl/~jmi/Dydaktyka/misc/kupony-klasyczne.pdf) to indicate which problems you are willing to present on the blackboard.\n",
    "\n",
    "$\\def\\R{{\\mathbb R}} \\def\\i{^{(i)}} \\def\\sjt{\\mathrm{s.t. }\\ }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TNgwkXtvJLT"
   },
   "source": [
    "# Problem 1 [2p]\n",
    "\n",
    "Let $X\\in \\R^{D\\times N}$ be a data matrix contianing $N$ $D$-dimensional points. Let $Y\\in\\R^{1\\times N}$ be the targets.\n",
    "\n",
    "We have seen that the least squares problem\n",
    "$$\n",
    "\\min_{\\Theta} \\frac{1}{2}(\\Theta^T X - Y)(\\Theta^T X - Y)^T\n",
    "$$\n",
    "has a closed form solution\n",
    "$$\n",
    "\\Theta^T{}^* = Y X^T(X X^T)^{-1}\n",
    "$$\n",
    "Where $X^+ = X^T(X X^T)^{-1}$ is the right [Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $X$:\n",
    "$$\n",
    "\\begin{split}\n",
    "\\Theta^T X &\\approx Y \\\\\n",
    "\\Theta^T X X^+ &\\approx Y X^{+} \\\\\n",
    "\\Theta^T &= Y X^{+}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The pseudoinverse also has another form (called a left inverse):\n",
    "$$\n",
    "X^+ = (X^T X)^{-1}X^T\n",
    "$$\n",
    "\n",
    "## P1.1 [0.5p]\n",
    "Say under which conditions the left and right pseudoinverses exist (when $X$ is a rectangular matrix only one index exists). Give examples of machine learning problems that could be solved using each inverse.\n",
    "\n",
    "### Ans:\n",
    "If A is m-by-n and n ≤ m, then A has a left inverse. \\\n",
    "If m ≤ n, then it has a right inverse. \\\n",
    "right -> lin reg\n",
    "\n",
    "## P1.2 [1p]\n",
    "Derive the left inverse by solving the regularized least squares problem\n",
    "$$\n",
    "\\min_\\Theta \\sum_i(\\Theta^T x\\i - y\\i)^2 + \\lambda\\Theta^T\\Theta\n",
    "$$\n",
    "with arificially introduced variables $\\epsilon\\i$ and constraints $\\epsilon\\i = \\Theta^T x\\i - y\\i$, then see what happens when $\\lambda\\rightarrow 0$.\n",
    "\n",
    "## P1.3 [0.5p]\n",
    "Show that the above dual formulation allows using Kernel functions with linear regression. Express the optimal solution using a weighed avegage of data samples. How many \"support vectors\" there are?\n",
    "\n",
    "NB: some authors call the kernelized linear regression the \"Least-Squares SVM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]]), array([[-7,  2, 14, ..., -2, 17, -4],\n",
       "        [ 0,  0,  6, ...,  0,  2,  2],\n",
       "        [-2,  0,  9, ...,  0,  6,  1],\n",
       "        ...,\n",
       "        [-2,  0,  3, ..., -1,  7,  0],\n",
       "        [-5,  1,  8, ..., -1, 14, -2],\n",
       "        [ 0,  0,  1, ...,  0,  2,  3]]), array([[162, 946, 130],\n",
       "        [140, 107, 742],\n",
       "        [225, 300, 870],\n",
       "        ...,\n",
       "        [388, 319, 167],\n",
       "        [528, 725,  48],\n",
       "        [476,  24, 549]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv, inv\n",
    "\n",
    "X = np.random.random((500, 3)) * 1000\n",
    "left = lambda x: inv(x.T @ x) @ x.T\n",
    "right = lambda x: x.T @ inv(x @ x.T)\n",
    "\n",
    "(left(X) @ X).astype(int), (X @ right(X)).astype(int), X.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6rmavsV2ifp"
   },
   "source": [
    "# Problem 2 (Bishop) [1p]\n",
    "\n",
    "Recall the nearest neighbor classifier. Show that the Euclidean distance\n",
    "$||x-y||^2$ can be expressed as a linear combination of dot-products. Using this \n",
    "formulation of the Euclidean distance, design a kernelized nearest neighbors method.\n",
    "\n",
    "## Ans:\n",
    "$$\n",
    "x^Tx + y^Ty - 2 * x^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxVvXJ70-JqP"
   },
   "source": [
    "# Problem 3 (Bishop) [1p]\n",
    "\n",
    "Recall the SVM training problem\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\min_{w,b} & \\frac{1}{2}w^T w \\\\\n",
    "\\sjt & y\\i(w^Tx\\i+b) \\geq 1\\qquad \\textrm{for all } i.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Show that the solution for the maximum margin hyperplane doesn't change when the $1$\n",
    "on the right-hand side of the contraints is replaced by any $\\gamma>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5n2hso7_YJu"
   },
   "source": [
    "# Problem 4 [2p]\n",
    "\n",
    "Show that if $\\kappa$ is the kernel matrix of a dataset, i.e. $\\kappa_{ij} = K(x\\i, x^{(j)}) = \\phi(x\\i)^T\\phi(x^{(j)})$ for some kernel function $K$ and an induced feature expansion function $\\phi$, then:\n",
    "\n",
    "## 4.1 [0.5p]\n",
    "$\\kappa$ is symmetric, i.e. $\\kappa = \\kappa^T$\n",
    "\n",
    "## 4.2 [1.5p]\n",
    "$\\kappa$ is positive semidefinite, i.e. for any vector $c$ we have \n",
    "$$\n",
    "c^T\\kappa c\\geq 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbP_4MV8UAIt"
   },
   "source": [
    "# Problem 5 [1p]\n",
    "\n",
    "Let $a$ and $b$ be two strings defined over an alphabet. $c$ is a substring of $a$ and $b$ if $a=xcz$ and $b=sct$ for some (possibly empty) strings $x, z, s, t$.\n",
    "\n",
    "Consider a function that counts the number of distinct substrings that are shared between two strings.\n",
    "\n",
    "Show that it is a kernel functon by showing how the feature expansion function $\\phi$ could be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rt0GFW6EU_Y3"
   },
   "source": [
    "# Problem 6 [1p bonus]\n",
    "\n",
    "Show how to kernelize logistic regression."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Homework3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
